{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15afeda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a218be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCORING = \"f1_micro\"\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "DATA_FOLDER = \"data/\"\n",
    "\n",
    "X_TRAIN_FILE_NAME = \"X_train_augmented.csv\"\n",
    "X_VALID_FILE_NAME = \"X_valid.csv\"\n",
    "\n",
    "Y_TRAIN_FILE_NAME = \"y_train_augmented.csv\"\n",
    "Y_VALID_FILE_NAME = \"y_valid.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45dd8e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(os.path.join(DATA_FOLDER, X_TRAIN_FILE_NAME), index_col=0)\n",
    "X_valid = pd.read_csv(os.path.join(DATA_FOLDER, X_VALID_FILE_NAME), index_col=0)\n",
    "\n",
    "y_train = pd.read_csv(os.path.join(DATA_FOLDER, Y_TRAIN_FILE_NAME), index_col=0).iloc[:,0]\n",
    "y_valid = pd.read_csv(os.path.join(DATA_FOLDER, Y_VALID_FILE_NAME), index_col=0).iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "931521d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10262, 2), (10262,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.values.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8848bfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11262, 2), (11262,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_full = pd.concat([X_train, X_valid])\n",
    "y = pd.concat([y_train, y_valid])\n",
    "\n",
    "X_full.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47e5af5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = [\n",
    "    Pipeline([\n",
    "        (\"vectorizer\", CountVectorizer()),\n",
    "        (\"model\", MultinomialNB())\n",
    "    ]),\n",
    "    Pipeline([\n",
    "        (\"vectorizer\", CountVectorizer()),\n",
    "        (\"model\", LinearSVC(max_iter=1500)) \n",
    "        # we do not standarize matrix (it will be nearly binary matrix)\n",
    "        # for tfidf - also mainly zeros and other values rathej small\n",
    "    ])\n",
    "]\n",
    "\n",
    "param_grids = [\n",
    "    \n",
    "    # combinations for the first model\n",
    "    [\n",
    "        # set 1\n",
    "        {\n",
    "            \"vectorizer\": [CountVectorizer()],\n",
    "            \"vectorizer__min_df\": [1, 5, 15],\n",
    "            \"vectorizer__max_df\": [1.0, 0.1, 0.05],\n",
    "            \"vectorizer__stop_words\": [None, [\"@anonymized_account\"]],  \n",
    "               #uwzględniamy wystąpienie \"@anonymized_account\" lub nie\n",
    "            \"vectorizer__token_pattern\": ['(?u)\\\\b\\\\w\\\\w+\\\\b', '(?u)\\\\b\\\\w+\\\\b'] \n",
    "            # uwzględniamy pojedyncze litery lub tylko slowa od 2 znakow\n",
    "        },\n",
    "        \n",
    "        # set 2\n",
    "        {\n",
    "            \"vectorizer\": [CountVectorizer()],\n",
    "            \"vectorizer__analyzer\": ['char'] \n",
    "                 # rozpatrujemy czestosci ZNAKOW zamiast slow\n",
    "        },\n",
    "        # set 3\n",
    "        {\n",
    "            \"vectorizer\": [CountVectorizer()],\n",
    "            \"vectorizer__analyzer\": ['char_wb'],\n",
    "            \"vectorizer__ngram_range\":[(1,2)]\n",
    "                # we consider chars and bigrams of chars\n",
    "        }\n",
    "    ],\n",
    "    \n",
    "    # combinations for the second model\n",
    "    {\n",
    "        \"vectorizer\": [CountVectorizer(), TfidfVectorizer()],\n",
    "        \"vectorizer__min_df\": [1, 5, 15],\n",
    "        \"vectorizer__max_df\": [1.0, 0.1, 0.05],\n",
    "        \"model__C\": [0.01, 0.1, 1, 10],\n",
    "        \"model__class_weight\": [None, \"balanced\", {0:1, 1:5, 2:20}]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd4b09bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to high complexiy of svm classifier\n",
    "# instead of performing cross validation \n",
    "# I used train-valid split on train set -\n",
    "# here is funtion built for using it as folds iterator in GridSearchCV\n",
    "\n",
    "n = X_full.shape[0]\n",
    "\n",
    "IND_TRAIN = np.arange(0, n-X_valid.shape[0], dtype=int)\n",
    "INT_VAL = np.arange(n-X_valid.shape[0], n, dtype=int)\n",
    "\n",
    "def train_valid_split():\n",
    "    yield IND_TRAIN, INT_VAL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b9b9854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: 'classic_ML_on_augmented_data' does not exist. Creating a new experiment\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment(\"classic_ML_on_augmented_data\")\n",
    "\n",
    "for text_variant in [\"text\", \"text_lemmatized\"]:\n",
    "\n",
    "    X = X_full[text_variant]\n",
    "    X_tr = X_train[text_variant]\n",
    "    X_val = X_valid[text_variant]\n",
    "    \n",
    "    \n",
    "    for pipeline, param_grid in zip(pipelines, param_grids):\n",
    "        with mlflow.start_run():\n",
    "            optimizer = GridSearchCV(pipeline, param_grid, \n",
    "                                     scoring = SCORING,\n",
    "                                     cv = train_valid_split(), \n",
    "                                     n_jobs=-1,\n",
    "                                     refit=True)\n",
    "            optimizer.fit(X, y)\n",
    "\n",
    "            best_model = pipeline\n",
    "            best_model.set_params(**optimizer.best_params_)\n",
    "            best_model.fit(X_tr, y_train)\n",
    "            # we calculate additional metrics only for best set of parameters        \n",
    "            y_val_pred = best_model.predict(X_val)\n",
    "            y_fitted = best_model.predict(X_tr)\n",
    "            \n",
    "            f1_macro_score = f1_score(y_valid, y_val_pred, average=\"macro\")\n",
    "            acc_score = accuracy_score(y_valid, y_val_pred)\n",
    "       \n",
    "            f1_micro_score_train = f1_score(y_train, y_fitted, average=\"micro\")\n",
    "            f1_macro_score_train = f1_score(y_train, y_fitted, average=\"macro\")\n",
    "            acc_score_train = accuracy_score(y_train, y_fitted)\n",
    "\n",
    "            mlflow.log_param(\"pipeline\", str(pipeline.steps))\n",
    "            mlflow.log_param(\"text_variant\", text_variant)\n",
    "            mlflow.log_param(\"best_params\", str(optimizer.best_params_))\n",
    "\n",
    "            mlflow.log_metric(\"f1_micro\", optimizer.best_score_)\n",
    "            mlflow.log_metric(\"f1_macro\", f1_macro_score)\n",
    "            mlflow.log_metric(\"accuracy\", acc_score)\n",
    "            mlflow.log_metric(\"f1_micro_train\", f1_micro_score_train)\n",
    "            mlflow.log_metric(\"f1_macro_train\", f1_macro_score_train)\n",
    "            mlflow.log_metric(\"accuracy_train\", acc_score_train)\n",
    "            \n",
    "            \n",
    "\n",
    "            with open(\"best_model.pkl\", \"wb\") as f:\n",
    "                pickle.dump(optimizer.best_estimator_, f)\n",
    "            mlflow.log_artifact(\"best_model.pkl\")\n",
    "            os.remove(\"best_model.pkl\")\n",
    "\n",
    "            with open(\"best_params.json\", \"w\") as f:\n",
    "                json.dump({k:str(v) for k,v in optimizer.best_params_.items()}, f)\n",
    "            mlflow.log_artifact(\"best_params.json\")\n",
    "            os.remove(\"best_params.json\")\n",
    "            \n",
    "            with open(\"optimization_history.json\", \"w\") as f:\n",
    "                json.dump({k:str(v) for k,v in optimizer.cv_results_.items()}, f)\n",
    "            mlflow.log_artifact(\"optimization_history.json\")\n",
    "            os.remove(\"optimization_history.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b69a54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
